{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets, load_from_disk, Dataset\n",
    "from src.sibyl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Concepts for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['ag_news', 'dbpedia_14', 'yahoo_answers_topics', 'imdb', 'yelp_polarity', 'amazon_polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using andi611/distilbert-base-uncased-ner-agnews to rationalize keyphrase selections.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c491d37f7d11486ca6770f261fb4c68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset d_bpedia14 (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\d_bpedia14\\dbpedia_14\\2.0.0\\7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fabriceyhc/bert-base-uncased-dbpedia_14 to rationalize keyphrase selections.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3ec0e421e641b0a89809e58beef04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_examples = 10\n",
    "\n",
    "results = []\n",
    "for task in tasks:\n",
    "    \n",
    "    # set task metadata\n",
    "    task_to_keys = {\n",
    "        \"ag_news\": {\"keys\": (\"text\", None), \"num_classes\": 4, \"task_type\": \"topic\"},\n",
    "        \"dbpedia_14\": {\"keys\": (\"text\", None), \"num_classes\": 14, \"task_type\": \"topic\"},\n",
    "        \"yahoo_answers_topics\": {\"keys\": (\"text\", None), \"num_classes\": 10, \"task_type\": \"topic\"},\n",
    "        \"imdb\": {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"},\n",
    "        \"yelp_polarity\":  {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"},\n",
    "        \"amazon_polarity\":  {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"}\n",
    "    }\n",
    "    sentence1_key, sentence2_key = task_to_keys[task][\"keys\"]\n",
    "    num_classes = task_to_keys[task][\"num_classes\"]\n",
    "    task_type = task_to_keys[task][\"task_type\"]\n",
    "    \n",
    "    dataset = load_dataset(task, split='train').select(range(num_examples))\n",
    "    \n",
    "    if task == \"yahoo_answers_topics\":\n",
    "        dataset = dataset.map(lambda example : {'text' : example['question_title'] + \" \" + \n",
    "                                                         example['question_content'] + \" \" +\n",
    "                                                         example['best_answer'],\n",
    "                                                'label': example['topic']})\n",
    "\n",
    "    if task in [\"dbpedia_14\", \"amazon_polarity\"]:\n",
    "        dataset = dataset.rename_column(\"content\", \"text\")\n",
    "        \n",
    "    transform = Concept2Sentence(dataset=task, return_concepts=True)\n",
    "\n",
    "    def apply_c2s_to_dataset(batch):\n",
    "        concepts, new_text = [], []\n",
    "        for data, target in zip(batch['text'], batch['label']):\n",
    "            c, t = transform(data, target)\n",
    "            concepts.append(c)\n",
    "            new_text.append(t)\n",
    "        return {\"text\": batch['text'], \"label\": batch['label'], \"concepts\": concepts, \"new_text\": new_text}\n",
    "\n",
    "    updated_dataset = dataset.map(apply_c2s_to_dataset, batched=True, batch_size=100)\n",
    "    results.append(updated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving C2S Concept Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using andi611/distilbert-base-uncased-ner-agnews to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = Concept2Sentence(dataset='ag_news', return_concepts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"What can I say? I know this movie from start to finish. It's hilarious. It's an strong link to my past and will change the way I view film in the future. Hypothetically speaking :) The down-fall? There's no Socrates Johnson!\"\n",
    "y = 1\n",
    "\n",
    "X = \"Hungarian GP, Friday Round-Up Fernando tenth and Jarno seventeenth but no cause for concern, while Pat Symonds explains the challenges of Fridays at the race.\"\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['gp', 'friday', 'tenth', 'race', 'pat', 'fernando'],\n",
       " 'fernando competes in a race on friday.')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts, new_sentence = t(X, y)\n",
    "concepts, new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = ['sudan', 'libyan', \"?!\", 'tenth', 'fernando', 'friday', 'fridays', 'flying']\n",
    "concept_lemmas = [lemmatizer.lemmatize(c) for c in concepts]\n",
    "new_concepts = [c for i, c in enumerate(concepts) if lemmatizer.lemmatize(c) not in concept_lemmas[:i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', '?!', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in concepts if c not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', '', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_depunct = [c.translate(str.maketrans('', '', string.punctuation)) for c in concepts]\n",
    "concept_depunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c1 for c1, c2 in zip(concepts, concept_depunct) if len(c2) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with num_sampled_INV=2 and num_sampled_SIB=0\n"
     ]
    }
   ],
   "source": [
    "task = \"imdb\"\n",
    "t = \"INV\"\n",
    "dataset = load_dataset(task, split=\"train[:20]\")\n",
    "\n",
    "task_to_keys = {\n",
    "        \"ag_news\": {\"keys\": (\"text\", None), \"num_classes\": 4, \"task_type\": \"topic\"},\n",
    "        \"dbpedia_14\": {\"keys\": (\"text\", None), \"num_classes\": 14, \"task_type\": \"topic\"},\n",
    "        \"yahoo_answers_topics\": {\"keys\": (\"text\", None), \"num_classes\": 10, \"task_type\": \"topic\"},\n",
    "        \"imdb\": {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"}\n",
    "    }\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task][\"keys\"]\n",
    "num_classes = task_to_keys[task][\"num_classes\"]\n",
    "task_type = task_to_keys[task][\"task_type\"]\n",
    "\n",
    "transform = None\n",
    "num_sampled_INV = 0\n",
    "num_sampled_SIB = 0\n",
    "label_type = \"soft\"\n",
    "\n",
    "if t == \"ORIG\":\n",
    "    label_type = \"hard\"\n",
    "elif t == \"INV\":\n",
    "    num_sampled_INV = 2\n",
    "    label_type = \"hard\"\n",
    "elif t == \"SIB\":\n",
    "    num_sampled_SIB = 2\n",
    "elif t == 'INVSIB':\n",
    "    num_sampled_INV = 1\n",
    "    num_sampled_SIB = 1\n",
    "    label_type = None\n",
    "    \n",
    "sibyl_collator = SibylCollator( \n",
    "        sentence1_key=sentence1_key,\n",
    "        sentence2_key=sentence2_key,\n",
    "        tokenize_fn=None, \n",
    "        transform=transform, \n",
    "        num_sampled_INV=num_sampled_INV, \n",
    "        num_sampled_SIB=num_sampled_SIB,\n",
    "        dataset=task,\n",
    "        task_type=task_type, \n",
    "        tran_type=None, \n",
    "        label_type=None,\n",
    "        one_hot=label_type != \"hard\",\n",
    "        transform_prob=0.5,\n",
    "        target_pairs=[],\n",
    "        target_prob=0.0,\n",
    "        reduce_mixed=False,\n",
    "        num_classes=num_classes,\n",
    "        return_tensors='np',\n",
    "        return_text=True\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb387dad0e16427787bdea48d5090f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sibyl_dataset_transform(batch):\n",
    "    new_batch = []\n",
    "    for data, target in zip(batch['text'], batch['label']):\n",
    "        new_batch.append({'text': data, 'label': target})\n",
    "    text, label = sibyl_collator(new_batch)\n",
    "    return {\"text\": text, \"label\": label}\n",
    "\n",
    "updated_dataset = dataset.map(sibyl_dataset_transform, batched=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(task, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I went and saw this movie last night after bei...      1\n",
       "1      Actor turned director Bill Paxton follows up h...      1\n",
       "2      As a recreational golfer with some knowledge o...      1\n",
       "3      I saw this film in a sneak preview, and it is ...      1\n",
       "4      Bill Paxton has taken the true story of the 19...      1\n",
       "...                                                  ...    ...\n",
       "24995  I occasionally let my kids watch this garbage ...      0\n",
       "24996  When all we have anymore is pretty much realit...      0\n",
       "24997  The basic genre is a thriller intercut with an...      0\n",
       "24998  Four things intrigued me as to this film - fir...      0\n",
       "24999  David Bryce's comments nearby are exceptionall...      0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fabriceyhc/bert-base-uncased-dbpedia_14 to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = Concept2Sentence(dataset='dbpedia_14', return_concepts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"Allez Oop is a 1934 American short comedy film starring Buster Keaton.\"]\n",
    "y = 1\n",
    "task_config = {'input_idx': [1],\n",
    "  'tran_type': 'INV',\n",
    "  'label_type': 'hard',\n",
    "  'task_name': 'topic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('american during a match against country in the summer of 1934.', 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_, y_ = t.transform_Xy(X, y, task_config)\n",
    "X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using yoshitomo-matsubara/bert-base-uncased-sst2 to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = ConceptMix(dataset='sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-a36969208caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnew_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "texts = [\"I hate how long loading the models takes to select better keyphrases.\",\n",
    "         \"I really love this movie a lot!\"]\n",
    "targets = [0, 1]\n",
    "batch = (texts, targets)\n",
    "new_text, new_target = t(batch, num_classes=2)\n",
    "print(new_text, new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_idx': [1, 0],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'},\n",
       " {'input_idx': [0, 1],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'},\n",
       " {'input_idx': [1, 1],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContractContractions().get_task_configs(task_name='similarity').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_label(y, soften=False, num_classes=2):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = soften_label(y, num_classes)\n",
    "    y = y[::-1]\n",
    "    if not soften:\n",
    "        y = np.argmax(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invert_label(0, soften=False, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.        , 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 0, 1, 1]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0. , 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
