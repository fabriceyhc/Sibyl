{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets, load_from_disk, Dataset\n",
    "from src.sibyl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_random = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring SibylCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SibylTransformer:\n",
    "    def __init__(self, task, num_classes=2, multiplier=1, num_INV=1, num_SIB=1):\n",
    "        self.task = task\n",
    "        self.num_classes = num_classes\n",
    "        self.multiplier = multiplier\n",
    "        self.num_INV = num_INV\n",
    "        self.num_SIB = num_SIB\n",
    "        \n",
    "        self.tran_df = init_transforms(task_name=self.task)\n",
    "        self.INV_fns = self.tran_df[self.tran_df['tran_type']=='INV']['tran_fn'].to_list()\n",
    "        self.SIB_fns = self.tran_df[self.tran_df['tran_type']=='SIB']['tran_fn'].to_list()\n",
    "        \n",
    "    def sample_transform(self, tran_type):\n",
    "        if tran_type == 'INV':\n",
    "            return np_random.choice(self.INV_fns)\n",
    "        else:\n",
    "            return np_random.choice(self.SIB_fns)\n",
    "        \n",
    "    def apply_transform(self, batch, transform):\n",
    "        if is_batched(transform):\n",
    "            (new_text, new_labels), meta = transform(\n",
    "                batch, \n",
    "                num_classes=self.num_classes\n",
    "            )\n",
    "            new_labels = [np.squeeze(one_hot_encode(y, self.num_classes)) for y in new_labels]\n",
    "            return new_text, new_labels\n",
    "        else:\n",
    "            new_text, new_labels = [], []\n",
    "            for X, y in zip(*batch):\n",
    "                X, y, meta = transform.transform_Xy(X, y)\n",
    "                new_text.append(X)\n",
    "                new_labels.append(y)  \n",
    "            return new_text, new_labels       \n",
    "                    \n",
    "    def __call__(self, batch):\n",
    "        new_text, new_labels = [], []\n",
    "        for _ in range(self.multiplier):\n",
    "            num_INV_applied, num_SIB_applied = 0, 0\n",
    "            while num_INV_applied < self.num_INV or num_SIB_applied < self.num_SIB:\n",
    "                \n",
    "                # sample transform\n",
    "                sample_prob = np.array([self.num_INV - num_INV_applied, self.num_SIB - num_SIB_applied])\n",
    "                sample_prob = sample_prob / sample_prob.sum()\n",
    "                tran_type = np_random.choice(['INV', 'SIB'], p=sample_prob)\n",
    "                transform = self.sample_transform(tran_type)\n",
    "                \n",
    "                # apply transform\n",
    "                text_, labels_ = self.apply_transform(batch, transform)\n",
    "                \n",
    "                new_text.extend(text_)\n",
    "                new_labels.extend(labels_)\n",
    "\n",
    "                num_INV_applied += 1 if tran_type == 'INV' else 0\n",
    "                num_SIB_applied += 1 if tran_type == 'SIB' else 0\n",
    "                \n",
    "        # format types\n",
    "        new_text = [str(x[0]) if type(x) == list else str(x) for x in new_text]\n",
    "        new_labels = [np.squeeze(y).tolist() if isinstance(y, (list, np.ndarray, torch.Tensor)) else y for y in new_labels]\n",
    "        \n",
    "        return new_text, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:10]\")\n",
    "dataset = dataset.rename_column('sentence', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SibylTransformer(\"sentiment\", num_INV = 2, num_SIB = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text, new_labels = [], []\n",
    "for batch in batcher(dataset, 5):\n",
    "    t_, l_ = t((batch['text'], batch['label']))\n",
    "    new_text.extend(t_)\n",
    "    new_labels.extend(l_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['show worn secretions from the filial units ',\n",
       "  'give no wit , only refresh gags ',\n",
       "  'that hate its antitype and curse something rather ugly about nonhuman nature ',\n",
       "  'change imperfectly satisfied to change the same throughout ',\n",
       "  'on the unregretting revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ',\n",
       "  'hide new secretions from the parental units  That being said, I hated it.',\n",
       "  \"contains no wit , only labored gags  That being said, I'm unhappy.\",\n",
       "  \"that loves its characters and communicates something rather beautiful about human nature  That being said, I couldn't be more upset.\",\n",
       "  \"remains utterly satisfied to remain the same throughout  That being said, I couldn't be more upset.\",\n",
       "  'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up  That being said, I hated it.',\n",
       "  'hide new secretions from the parental units',\n",
       "  'contains no wit , only labored gags',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature',\n",
       "  'remains utterly satisfied to remain the same throughout',\n",
       "  'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up',\n",
       "  'hide ðŸ†• secretions from the parental units ',\n",
       "  'contains ðŸ‘Ž wit , only labored gags ',\n",
       "  'that loves its ðŸ”£ and communicates something rather beautiful about ðŸ‘¨\\u200dðŸ‘¦\\u200dðŸ‘¦ ðŸ¦š ',\n",
       "  'remains utterly ðŸ˜† ðŸ‡¹ðŸ‡´ remain the same throughout ',\n",
       "  'ðŸ”› the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge ðŸ†™ ',\n",
       "  'b\"that \\'s far too tragic to merit such superficial treatment  a depressed fifteen-year-old \\'s suicidal poetry \"',\n",
       "  'b\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .  are more deeply thought through than in most ` right-thinking \\' films \"',\n",
       "  'b\"of saucy  are more deeply thought through than in most ` right-thinking \\' films \"',\n",
       "  'b\"a depressed fifteen-year-old \\'s suicidal poetry  a depressed fifteen-year-old \\'s suicidal poetry \"',\n",
       "  'b\"are more deeply thought through than in most ` right-thinking \\' films  a depressed fifteen-year-old \\'s suicidal poetry \"',\n",
       "  \"that 's far too tragic to merit such superficial hospitalization \",\n",
       "  'picket that the manageress of such hollywood blockbusters as jingo backgammon can still unhitch out a small , personal positive with an emotional wallop . ',\n",
       "  'of saucy ',\n",
       "  \"a depressed fifteen-year-old 's suicidal epos \",\n",
       "  \"are more deeply know through than in most ` right-thinking ' negative \",\n",
       "  \"that 's far too tragic to merit such suerficial treatment \",\n",
       "  'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional walop . ',\n",
       "  'of sauc ',\n",
       "  \"a depressed fifteen-year-old 's suicdal poetry \",\n",
       "  \"are more deeply thought through than in most ` riht-thinking ' films \",\n",
       "  \"that 's far too tragic to merit such superficial treatment  That being said, I loved it.\",\n",
       "  \"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .  That being said, I'm pleased.\",\n",
       "  \"of saucy  That being said, I couldn't be happier.\",\n",
       "  \"a depressed fifteen-year-old 's suicidal poetry  That being said, I couldn't be happier.\",\n",
       "  \"are more deeply thought through than in most ` right-thinking ' films  That being said, I loved it.\"],\n",
       " [1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  [0.75, 0.25],\n",
       "  [0.75, 0.25],\n",
       "  [0.25, 0.75],\n",
       "  [0.75, 0.25],\n",
       "  [0.75, 0.25],\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.4067796610169492, 0.5932203389830508],\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  [0.75, 0.25],\n",
       "  [0.25, 0.75],\n",
       "  [0.25, 0.75],\n",
       "  [0.75, 0.25],\n",
       "  [0.25, 0.75]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Concepts for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['ag_news', 'dbpedia_14', 'yahoo_answers_topics', 'imdb', 'yelp_polarity', 'amazon_polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using andi611/distilbert-base-uncased-ner-agnews to rationalize keyphrase selections.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c491d37f7d11486ca6770f261fb4c68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset d_bpedia14 (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\d_bpedia14\\dbpedia_14\\2.0.0\\7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fabriceyhc/bert-base-uncased-dbpedia_14 to rationalize keyphrase selections.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3ec0e421e641b0a89809e58beef04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_examples = 10\n",
    "\n",
    "results = []\n",
    "for task in tasks:\n",
    "    \n",
    "    # set task metadata\n",
    "    task_to_keys = {\n",
    "        \"ag_news\": {\"keys\": (\"text\", None), \"num_classes\": 4, \"task_type\": \"topic\"},\n",
    "        \"dbpedia_14\": {\"keys\": (\"text\", None), \"num_classes\": 14, \"task_type\": \"topic\"},\n",
    "        \"yahoo_answers_topics\": {\"keys\": (\"text\", None), \"num_classes\": 10, \"task_type\": \"topic\"},\n",
    "        \"imdb\": {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"},\n",
    "        \"yelp_polarity\":  {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"},\n",
    "        \"amazon_polarity\":  {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"}\n",
    "    }\n",
    "    sentence1_key, sentence2_key = task_to_keys[task][\"keys\"]\n",
    "    num_classes = task_to_keys[task][\"num_classes\"]\n",
    "    task_type = task_to_keys[task][\"task_type\"]\n",
    "    \n",
    "    dataset = load_dataset(task, split='train').select(range(num_examples))\n",
    "    \n",
    "    if task == \"yahoo_answers_topics\":\n",
    "        dataset = dataset.map(lambda example : {'text' : example['question_title'] + \" \" + \n",
    "                                                         example['question_content'] + \" \" +\n",
    "                                                         example['best_answer'],\n",
    "                                                'label': example['topic']})\n",
    "\n",
    "    if task in [\"dbpedia_14\", \"amazon_polarity\"]:\n",
    "        dataset = dataset.rename_column(\"content\", \"text\")\n",
    "        \n",
    "    transform = Concept2Sentence(dataset=task, return_concepts=True)\n",
    "\n",
    "    def apply_c2s_to_dataset(batch):\n",
    "        concepts, new_text = [], []\n",
    "        for data, target in zip(batch['text'], batch['label']):\n",
    "            c, t = transform(data, target)\n",
    "            concepts.append(c)\n",
    "            new_text.append(t)\n",
    "        return {\"text\": batch['text'], \"label\": batch['label'], \"concepts\": concepts, \"new_text\": new_text}\n",
    "\n",
    "    updated_dataset = dataset.map(apply_c2s_to_dataset, batched=True, batch_size=100)\n",
    "    results.append(updated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving C2S Concept Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [{'len':len(x['text'].split())} for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13568</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16479</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16854</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14807</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23609</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15633</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21936</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       len\n",
       "13568   10\n",
       "16479   10\n",
       "16854   11\n",
       "6408    12\n",
       "14807   12\n",
       "23609   14\n",
       "15633   15\n",
       "21936   17\n",
       "4131    17\n",
       "4996    18"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(stats).sort_values('len').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fabriceyhc/bert-base-uncased-imdb to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = Concept2Sentence(dataset='imdb', return_concepts=True)\n",
    "antonymizer = ChangeAntonym()\n",
    "synonymizer = ChangeSynonym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT! 0 ['worse'] worse at the end of the day\n"
     ]
    }
   ],
   "source": [
    "idx = 21936\n",
    "X = dataset[idx]['text']\n",
    "y = dataset[idx]['label']\n",
    "\n",
    "concepts, new_sentence = t(X, y)\n",
    "print(X, y, concepts, new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unregretful'] people are disappointed by the lack of engagement.\n"
     ]
    }
   ],
   "source": [
    "new_concepts = [antonymizer(c) for c in concepts]\n",
    "new_sentence = t.generate_text_from_concepts(new_concepts)\n",
    "print(new_concepts, new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tough', 'dazed'] The man is dazed and trying to get out of bed.\n"
     ]
    }
   ],
   "source": [
    "new_concepts = [synonymizer(c) for c in ['worse', 'stupid']]\n",
    "new_sentence = t.generate_text_from_concepts(new_concepts)\n",
    "print(new_concepts, new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"What can I say? I know this movie from start to finish. It's hilarious. It's an strong link to my past and will change the way I view film in the future. Hypothetically speaking :) The down-fall? There's no Socrates Johnson!\"\n",
    "y = 1\n",
    "\n",
    "X = \"Hungarian GP, Friday Round-Up Fernando tenth and Jarno seventeenth but no cause for concern, while Pat Symonds explains the challenges of Fridays at the race.\"\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['gp', 'friday', 'tenth', 'race', 'pat', 'fernando'],\n",
       " 'fernando competes in a race on friday.')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts, new_sentence = t(X, y)\n",
    "concepts, new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = ['sudan', 'libyan', \"?!\", 'tenth', 'fernando', 'friday', 'fridays', 'flying']\n",
    "concept_lemmas = [lemmatizer.lemmatize(c) for c in concepts]\n",
    "new_concepts = [c for i, c in enumerate(concepts) if lemmatizer.lemmatize(c) not in concept_lemmas[:i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', '?!', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in concepts if c not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', '', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_depunct = [c.translate(str.maketrans('', '', string.punctuation)) for c in concepts]\n",
    "concept_depunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudan', 'libyan', 'tenth', 'fernando', 'friday', 'fridays', 'flying']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c1 for c1, c2 in zip(concepts, concept_depunct) if len(c2) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibylCollator initialized with num_sampled_INV=2 and num_sampled_SIB=0\n"
     ]
    }
   ],
   "source": [
    "task = \"imdb\"\n",
    "t = \"INV\"\n",
    "dataset = load_dataset(task, split=\"train[:20]\")\n",
    "\n",
    "task_to_keys = {\n",
    "        \"ag_news\": {\"keys\": (\"text\", None), \"num_classes\": 4, \"task_type\": \"topic\"},\n",
    "        \"dbpedia_14\": {\"keys\": (\"text\", None), \"num_classes\": 14, \"task_type\": \"topic\"},\n",
    "        \"yahoo_answers_topics\": {\"keys\": (\"text\", None), \"num_classes\": 10, \"task_type\": \"topic\"},\n",
    "        \"imdb\": {\"keys\": (\"text\", None), \"num_classes\": 2, \"task_type\": \"sentiment\"}\n",
    "    }\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task][\"keys\"]\n",
    "num_classes = task_to_keys[task][\"num_classes\"]\n",
    "task_type = task_to_keys[task][\"task_type\"]\n",
    "\n",
    "transform = None\n",
    "num_sampled_INV = 0\n",
    "num_sampled_SIB = 0\n",
    "label_type = \"soft\"\n",
    "\n",
    "if t == \"ORIG\":\n",
    "    label_type = \"hard\"\n",
    "elif t == \"INV\":\n",
    "    num_sampled_INV = 2\n",
    "    label_type = \"hard\"\n",
    "elif t == \"SIB\":\n",
    "    num_sampled_SIB = 2\n",
    "elif t == 'INVSIB':\n",
    "    num_sampled_INV = 1\n",
    "    num_sampled_SIB = 1\n",
    "    label_type = None\n",
    "    \n",
    "sibyl_collator = SibylCollator( \n",
    "        sentence1_key=sentence1_key,\n",
    "        sentence2_key=sentence2_key,\n",
    "        tokenize_fn=None, \n",
    "        transform=transform, \n",
    "        num_sampled_INV=num_sampled_INV, \n",
    "        num_sampled_SIB=num_sampled_SIB,\n",
    "        dataset=task,\n",
    "        task_type=task_type, \n",
    "        tran_type=None, \n",
    "        label_type=None,\n",
    "        one_hot=label_type != \"hard\",\n",
    "        transform_prob=0.5,\n",
    "        target_pairs=[],\n",
    "        target_prob=0.0,\n",
    "        reduce_mixed=False,\n",
    "        num_classes=num_classes,\n",
    "        return_tensors='np',\n",
    "        return_text=True\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb387dad0e16427787bdea48d5090f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sibyl_dataset_transform(batch):\n",
    "    new_batch = []\n",
    "    for data, target in zip(batch['text'], batch['label']):\n",
    "        new_batch.append({'text': data, 'label': target})\n",
    "    text, label = sibyl_collator(new_batch)\n",
    "    return {\"text\": text, \"label\": label}\n",
    "\n",
    "updated_dataset = dataset.map(sibyl_dataset_transform, batched=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(task, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I went and saw this movie last night after bei...      1\n",
       "1      Actor turned director Bill Paxton follows up h...      1\n",
       "2      As a recreational golfer with some knowledge o...      1\n",
       "3      I saw this film in a sneak preview, and it is ...      1\n",
       "4      Bill Paxton has taken the true story of the 19...      1\n",
       "...                                                  ...    ...\n",
       "24995  I occasionally let my kids watch this garbage ...      0\n",
       "24996  When all we have anymore is pretty much realit...      0\n",
       "24997  The basic genre is a thriller intercut with an...      0\n",
       "24998  Four things intrigued me as to this film - fir...      0\n",
       "24999  David Bryce's comments nearby are exceptionall...      0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fabriceyhc/bert-base-uncased-dbpedia_14 to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = Concept2Sentence(dataset='dbpedia_14', return_concepts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"Allez Oop is a 1934 American short comedy film starring Buster Keaton.\"]\n",
    "y = 1\n",
    "task_config = {'input_idx': [1],\n",
    "  'tran_type': 'INV',\n",
    "  'label_type': 'hard',\n",
    "  'task_name': 'topic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('american during a match against country in the summer of 1934.', 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_, y_ = t.transform_Xy(X, y, task_config)\n",
    "X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using yoshitomo-matsubara/bert-base-uncased-sst2 to rationalize keyphrase selections.\n"
     ]
    }
   ],
   "source": [
    "t = ConceptMix(dataset='sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-a36969208caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnew_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "texts = [\"I hate how long loading the models takes to select better keyphrases.\",\n",
    "         \"I really love this movie a lot!\"]\n",
    "targets = [0, 1]\n",
    "batch = (texts, targets)\n",
    "new_text, new_target = t(batch, num_classes=2)\n",
    "print(new_text, new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_idx': [1, 0],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'},\n",
       " {'input_idx': [0, 1],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'},\n",
       " {'input_idx': [1, 1],\n",
       "  'tran_type': 'INV',\n",
       "  'label_type': 'hard',\n",
       "  'task_name': 'similarity'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContractContractions().get_task_configs(task_name='similarity').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_label(y, soften=False, num_classes=2):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = soften_label(y, num_classes)\n",
    "    y = y[::-1]\n",
    "    if not soften:\n",
    "        y = np.argmax(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invert_label(0, soften=False, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.        , 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 0, 1, 1]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0. , 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
